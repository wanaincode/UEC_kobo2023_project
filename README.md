# ç”»åƒèªè­˜æŠ€è¡“ã‚’æ´»ç”¨ã—ãŸå†·è”µåº«å†…é£Ÿæè‡ªå‹•åˆ¤åˆ¥ã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™º (æ—¥æœ¬èªç‰ˆ)   
## 1. ã¯ã˜ã‚ã«

- ğŸ† æƒ…å ±å‡¦ç†å­¦ä¼š ç¬¬86å›å¤§ä¼š å­¦ç”Ÿå¥¨åŠ±è³ å—è³ä½œå“  
- é›»æ°—é€šä¿¡å¤§å­¦ã€Œæƒ…å ±å·¥å­¦å·¥æˆ¿ã€æœ€çµ‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
- èª¿å¸ƒç¥­2023ã«å±•ç¤º    

### 1.1 ãƒ‡ãƒ¢  
[![ãƒ‡ãƒ¢ã‚’è¦‹ã‚‹](https://img.youtube.com/vi/WM_rVHsI6sQ/maxresdefault.jpg)](https://www.youtube.com/watch?v=WM_rVHsI6sQ)  


### 1.2 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±  
- **ãƒ†ãƒ¼ãƒ:** ã€Œå·¨å¤§ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã“ãªã›ï¼å¤§è¦æ¨¡æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸç”»åƒèªè­˜ãƒ»ç”Ÿæˆã€  
- **è‘—è€…:**  
  - ç‹ å­Ÿçª (Mengchi Wang)  
  - é™³ ä¿Šæ–‡ (Junwen Chen) â€“ TAã•ã‚“  
  - æŸ³äº• å•“å¸ (Prof. Keiji Yanai) â€“ æŒ‡å°æ•™æˆ  


### 1.3 æ¦‚è¦  
æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€æœ€å…ˆç«¯ã®è¦–è¦šã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã€ç”»åƒå†…ã®é£Ÿæã‚’æ¤œå‡ºãƒ»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ãƒ»èªè­˜ã—ã€å¿…è¦ã«å¿œã˜ã¦OpenAI APIã§åç§°ã‚„ã‚«ãƒ­ãƒªãƒ¼æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚

ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:  
- **GroundingDINO** â€“ ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªç‰©ä½“æ¤œå‡º  
- **Segment Anything (SAM)** â€“ é«˜ç²¾åº¦ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³  
- **CLIP** â€“ ç‰¹å¾´æŠ½å‡ºã¨é¡ä¼¼åº¦ãƒãƒƒãƒãƒ³ã‚°  
- **OpenAI GPT-4 Vision**ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ â€“ é£Ÿæåã¨ã‚«ãƒ­ãƒªãƒ¼ã®å–å¾—  

ç”¨é€”ä¾‹:  
- é£Ÿäº‹è¨˜éŒ²  
- ARé£Ÿæãƒ©ãƒ™ãƒªãƒ³ã‚°  
- ãƒ€ã‚¤ã‚¨ãƒƒãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ   

---

## 2. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£  

     +----------------+          +---------------------+
     |   å…¥åŠ›ç”»åƒ     |  --->    |  GroundingDINO      |
     +----------------+          +---------------------+
                                          |
                                          v
                            +---------------------------+
                            | Segment Anything (SAM)    |
                            +---------------------------+
                                          |
                                          v
                            +---------------------------+
                            | CLIPç‰¹å¾´æŠ½å‡º              |
                            +---------------------------+
                                          |
                  +--------------------+  v
                  | ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°      | ---> ç‰©ä½“ãƒ©ãƒ™ãƒ«
                  +--------------------+
                                          |
                                          v
                        ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰OpenAI GPT-4 APIã‚’å‘¼ã³å‡ºã—
                        ç‰©ä½“åãƒ»ã‚«ãƒ­ãƒªãƒ¼ã‚’å–å¾—



### 2.1 ç‰¹å¾´
- ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ç‰©ä½“æ¤œå‡ºï¼ˆGroundingDINOï¼‰  
- ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSAMï¼‰  
- åŸ‹ã‚è¾¼ã¿ç‰¹å¾´ã¨é¡ä¼¼åº¦æ¤œç´¢ï¼ˆCLIPï¼‰  
- ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆèªè­˜ï¼ˆç™»éŒ²æ¸ˆã¿ç‰¹å¾´ã‚’ä½¿ç”¨ï¼‰  
- ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šAPIã«ã‚ˆã‚‹é£Ÿæèª¬æ˜ã¨ã‚«ãƒ­ãƒªãƒ¼æƒ…å ±  
- ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨ãƒ©ãƒ™ãƒ«ä»˜ãã®å¯è¦–åŒ–çµæœå‡ºåŠ›  

---

## 3. ä½¿ã„æ–¹  

### 3.1 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«  
```bash
git clone https://github.com/wanaincode/KOBO2023_project.git
cd KOBO2023_project
pip install -r requirements.txt
```
ä»¥ä¸‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:  
- GroundingDINO  
- SAM (ViT-H)  
- CLIP (ViT-B/32)  

ã™ã¹ã¦ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ `data/` ãƒ•ã‚©ãƒ«ãƒ€å†…ã®èª­ã¿æ›¸ãã‚’æƒ³å®šã—ã¦ãŠã‚Šã€åˆå›å®Ÿè¡Œæ™‚ã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ã€‚


### 3.2 åˆ©ç”¨æ–¹æ³•  

åˆ©ç”¨ç›®çš„ã«å¿œã˜ã¦ä»¥ä¸‹ã®ãƒ¢ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„:

| ã‚¹ã‚¯ãƒªãƒ—ãƒˆ                 | ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ— | æ¤œå‡ºãƒ»èªè­˜å†…å®¹                             | å‡ºåŠ›ãƒ»ãƒ‘ã‚¹ (`data/` ä»¥ä¸‹)                                  | å‚™è€ƒ |
|---------------------------|------------------------|------------------------------------------|------------------------------------------------------------|-------|
| `demo_ui.py` (æ¨å¥¨)        | OpenCV GUI (ã‚¦ã‚§ãƒ–ã‚«ãƒ¡ãƒ©) | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºâ†’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆâ†’åŸ‹ã‚è¾¼ã¿â†’ãƒãƒƒãƒãƒ³ã‚° | `recognized_results/` ã«ä¿å­˜ã€‚`sample_features/` ã¾ãŸã¯ `extract_saved_obj_feature/` ä½¿ç”¨ | ãƒã‚¦ã‚¹ãƒœã‚¿ãƒ³: æŠ½è±¡åŒ– / èªè­˜; Enterã§ã‚­ãƒ£ãƒ—ãƒãƒ£ |
| `demo_no_ui.py`            | ã‚¿ãƒ¼ãƒŸãƒŠãƒ« + ã‚¦ã‚§ãƒ–ã‚«ãƒ¡ãƒ© | ã‚­ãƒ¼æ“ä½œã«ã‚ˆã‚‹ãƒãƒƒãƒ/ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆèªè­˜ | åŒã˜ã `data/` é…ä¸‹ï¼ˆ`saved_obj_img/`, `extract_saved_obj_feature/` ç­‰ï¼‰ | è»½é‡ã€GUIãªã— |
| `demo_basic.py`            | æœ€å°é™ (ã‚¦ã‚§ãƒ–ã‚«ãƒ¡ãƒ©)     | CLIPé¡ä¼¼åº¦ã®åŸºæœ¬ä¾‹                      | `data/` ä»¥ä¸‹ã«ç°¡æ˜“å‡ºåŠ›                                     | ç°¡æ˜“ãƒ†ã‚¹ãƒˆç”¨ |


#### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
KOBO2023_project/
â”œâ”€â”€ src/                       # ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ api_get_food_info.py
â”‚   â”œâ”€â”€ demo_ui.py
â”‚   â”œâ”€â”€ demo_no_ui.py
â”‚   â”œâ”€â”€ opencv_frame.py
â”‚   â””â”€â”€ sam_generate_obj_mask.py
â”œâ”€â”€ notebooks/                 # Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
â”‚   â”œâ”€â”€ SAM_demo.ipynb
â”‚   â”œâ”€â”€ clip_test.ipynb
â”‚   â””â”€â”€ ground_dino_sam_clip.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ saved_obj_img/         # ã‚­ãƒ£ãƒ—ãƒãƒ£æ¸ˆã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”»åƒï¼ˆåå‰åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ï¼‰
â”‚   â”œâ”€â”€ extract_saved_obj_feature/  # è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸãƒã‚¹ã‚¯/åˆ‡ã‚Šå‡ºã— & obj_features.pt
â”‚   â”œâ”€â”€ sample_features/       # ãƒãƒ³ãƒ‰ãƒ«æ¸ˆã¿ã‚µãƒ³ãƒ—ãƒ«obj_features.ptï¼ˆè©¦ç”¨ç”¨ï¼‰
â”‚   â”œâ”€â”€ test_img/              # èªè­˜æ™‚ã«è‡ªå‹•ä¿å­˜ã•ã‚Œã‚‹test.jpg
â”‚   â””â”€â”€ recognized_results/    # äºˆæ¸¬çµæœï¼ˆæ³¨é‡ˆä»˜ãç”»åƒï¼‰
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> æ³¨æ„: ãƒªãƒã‚¸ãƒˆãƒªç›´ä¸‹ã‹ã‚‰ç›´æ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã¯ï¼ˆ`src/`ã‚’çµŒç”±ã—ãªã„å ´åˆï¼‰ã€ã‚³ãƒãƒ³ãƒ‰ã‚’é©å®œèª¿æ•´ã—ã¦ãã ã•ã„ï¼ˆä¾‹: `python demo_ui.py`ï¼‰ã€‚ã™ã¹ã¦ã®å…¥å‡ºåŠ›ã¯ `data/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ã‚ã‚‹ã€‚

### 3.3 GUIç‰ˆã«ã‚ˆã‚‹èªè­˜ã®å®Ÿè¡Œ (`demo_ui.py`)

ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«è¦–è¦šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å¾—ãªãŒã‚‰å®Ÿè¡Œã™ã‚‹æ¨å¥¨æ–¹æ³•ã€‚ã‚¦ã‚§ãƒ–ã‚«ãƒ¡ãƒ©ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

#### ğŸ’¡ ã™ãã«è©¦ã™  
ãƒãƒ³ãƒ‰ãƒ«æ¸ˆã¿ã®ã‚µãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡ã‚’ä½¿ã£ã¦ã™ãã«èªè­˜ã‚’è©¦ã›ã¾ã™:  
- ã¾ãš `data/sample_features/obj_features.pt` ã‚’å„ªå…ˆä½¿ç”¨;  
- ãªã‘ã‚Œã° `data/extract_saved_obj_feature/obj_features.pt` ã‚’ä½¿ç”¨ã€‚  

ã‚¦ã‚§ãƒ–ã‚«ãƒ¡ãƒ©ã‹ã‚‰ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¯ `data/test_img/` ã«ä¿å­˜ã•ã‚Œã€æ³¨é‡ˆä»˜ãçµæœã¯ `data/recognized_results/` ã«æ›¸ãè¾¼ã¾ã‚Œã‚‹ã€‚

#### ğŸ’¡ è‡ªåˆ†ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™»éŒ²ã™ã‚‹  
1. GUIã‚’èµ·å‹•:  
   ```bash
   python src/demo_ui.py
   ```  
2. ä¸‹éƒ¨ã®å…¥åŠ›æ¬„ã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåï¼ˆä¾‹: `apple`ï¼‰ã‚’å…¥åŠ›ã—ã€Enterã‚­ãƒ¼ã‚’æŠ¼ã™ã€‚  
   3æ–¹å‘ï¼ˆ`x`, `y`, `z`ï¼‰ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã€`data/saved_obj_img/<object_name>/` ã«ä¿å­˜ã€‚  
3. ä»–ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚‚åŒæ§˜ã«ç™»éŒ²ã€‚  
4. å³ãƒ‘ãƒãƒ«ã®ã€ŒæŠ½è±¡åŒ–ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ç‰¹å¾´ã‚’æŠ½å‡ºã—ã€`data/extract_saved_obj_feature/obj_features.pt` ã‚’ç”Ÿæˆã€‚  
5. ã€Œèªè­˜ã€ãƒœã‚¿ãƒ³ã§æ¤œå‡ºãƒ»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ»é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¡¨ç¤ºã‚’å®Ÿè¡Œã€‚çµæœã¯ `data/recognized_results/` ã«ä¿å­˜ã€‚

#### ğŸ’¡ æ“ä½œæ–¹æ³•  
- **Enter**: å…¥åŠ›ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåã‚’ç¢ºå®šã—ã€ç”»åƒã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹  
- **ãƒœã‚¿ãƒ³**: `æŠ½è±¡åŒ–`ï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰ã€`èªè­˜`ï¼ˆèªè­˜å®Ÿè¡Œï¼‰ã€`çµ‚äº†`ï¼ˆçµ‚äº†ï¼‰  
- **ESC**: å¾…æ©Ÿä¸­ã«çµ‚äº†  

#### ğŸ’¡ å‡ºåŠ›  
- å­¦ç¿’ç”¨ç”»åƒ: `data/saved_obj_img/`  
- æŠ½å‡ºç‰¹å¾´: `data/extract_saved_obj_feature/`  
- ã‚µãƒ³ãƒ—ãƒ«ç‰¹å¾´: `data/sample_features/`  
- ãƒ†ã‚¹ãƒˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: `data/test_img/`  
- èªè­˜çµæœ: `data/recognized_results/`  

---

# English Version

# AI Food Recognizer: Refrigerator Ingredient Recognition System  
## 1. Intro

- ğŸ† Student Encouragement Award at the 86th Annual Conference of the Information Processing Society of Japan (IPSJ)
- Final project for **"æƒ…å ±å·¥å­¦å·¥æˆ¿ (Information Engineering Workshop)"**, The University of Electro-Communications (é›»æ°—é€šä¿¡å¤§å­¦)  
- Exhibited at Chofu Festival 2023


### 1.1 Demo  
[![Watch the demo](https://img.youtube.com/vi/WM_rVHsI6sQ/maxresdefault.jpg)](https://www.youtube.com/watch?v=WM_rVHsI6sQ)  


### 1.2 Project Info  
- **Theme:** ã€Œå·¨å¤§ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã“ãªã›ï¼å¤§è¦æ¨¡æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸç”»åƒèªè­˜ãƒ»ç”Ÿæˆã€  
- **Authors:**  
  - Mengchi Wang (ç‹ å­Ÿçª)  
  - Junwen Chen (é™³ ä¿Šæ–‡) â€“ Teaching Assistant  
  - Prof. Keiji Yanai (æŸ³äº• å•“å¸) â€“ Faculty Advisor  


### 1.3 Overview  
This project integrates multiple state-of-the-art vision and language models to detect, segment, and recognize food items in images, and optionally fetch their names and calorie information using the OpenAI API.

Models used:  
- **GroundingDINO** â€“ open-vocabulary object detection with text prompts  
- **Segment Anything (SAM)** â€“ precise image segmentation  
- **CLIP** â€“ feature embedding and similarity matching  
- **OpenAI GPT-4 Vision** (optional) â€“ retrieve food names & calories  

Ideal for:  
- Food logging  
- AR food labeling  
- Diet tracking systems  

---

## 2. Architecture  

     +----------------+          +---------------------+
     |   Input Image  |  --->    |  GroundingDINO      |
     +----------------+          +---------------------+
                                          |
                                          v
                            +---------------------------+
                            | Segment Anything (SAM)    |
                            +---------------------------+
                                          |
                                          v
                            +---------------------------+
                            | CLIP Feature Extraction   |
                            +---------------------------+
                                          |
                  +--------------------+  v
                  | Feature Matching   | ---> Object Label
                  +--------------------+
                                          |
                                          v
                        (Optional) Call OpenAI GPT-4 API
                        to retrieve object name & calorie



### 2.1 Key Features
- Text-based object detection (GroundingDINO)  
- Image segmentation (SAM)  
- Embedding & similarity search (CLIP)  
- One-shot recognition with registered features  
- Optional: API-based food description & calories  
- Export visual results with bounding boxes & labels  

---

## 3. Getting Started  

### 3.1 Installation  
```bash
git clone https://github.com/wanaincode/KOBO2023_project.git
cd KOBO2023_project
pip install -r requirements.txt
```
Download checkpoints for:  
- GroundingDINO  
- SAM (ViT-H)  
- CLIP (ViT-B/32)  

All scripts expect read/write under `data/` (created automatically on first run).


### 3.2 Usage  

Choose one of the following modes depending on your preference and use case:

| Script                     | Interaction Type       | Detection & Recognition                      | Output & Paths (under `data/`)                                  | Notes |
|---------------------------|------------------------|-----------------------------------------------|------------------------------------------------------------------|-------|
| `demo_ui.py` (recommended)| OpenCV GUI (webcam)    | Real-time detect â†’ segment â†’ embed â†’ match    | Saves to `recognized_results/`; uses `sample_features/` or `extract_saved_obj_feature/` | Mouse buttons: Abstract / Recognize; Enter to capture |
| `demo_no_ui.py`           | Terminal + webcam      | Batch/one-shot via key commands               | Same `data/` layout (`saved_obj_img/`, `extract_saved_obj_feature/`, etc.) | Lightweight, no GUI |
| `demo_basic.py`           | Minimal (webcam)       | Basic CLIP similarity example                 | Writes simple outputs under `data/`                              | For quick testing |


#### Directory Structure

```
KOBO2023_project/
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ api_get_food_info.py
â”‚   â”œâ”€â”€ demo_ui.py
â”‚   â”œâ”€â”€ demo_no_ui.py
â”‚   â”œâ”€â”€ opencv_frame.py
â”‚   â””â”€â”€ sam_generate_obj_mask.py
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â”œâ”€â”€ SAM_demo.ipynb
â”‚   â”œâ”€â”€ clip_test.ipynb
â”‚   â””â”€â”€ ground_dino_sam_clip.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ saved_obj_img/         # Your captured object images (organized by object name)
â”‚   â”œâ”€â”€ extract_saved_obj_feature/  # Auto-generated masks/crops & obj_features.pt
â”‚   â”œâ”€â”€ sample_features/       # Bundled sample obj_features.pt (for quick try)
â”‚   â”œâ”€â”€ test_img/              # Auto-saved test.jpg when recognizing
â”‚   â””â”€â”€ recognized_results/    # Prediction results (annotated images)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> Note: If you are running scripts directly from the repo root (not using `src/`), adjust the command accordingly (e.g., `python demo_ui.py`). All I/O paths are under the `data/` directory.

### 3.3 Running Recognition with `demo_ui.py` (GUI Version)

This is the recommended way to run the pipeline interactively with visual feedback. The GUI uses your webcam.

#### ğŸ’¡ Quick Try  
You can try recognition immediately using the bundled sample features:  
- The app will prefer `data/sample_features/obj_features.pt` if present;  
- Otherwise, it will fall back to `data/extract_saved_obj_feature/obj_features.pt`.  

A snapshot from the webcam will be saved under `data/test_img/`, and annotated results will be written to `data/recognized_results/`.

#### ğŸ’¡ Register Your Own Objects  
1. Run the GUI:
   ```bash
   python src/demo_ui.py
   ```  
2. In the bottom input box, type an **object name** (e.g., `apple`) and press **Enter**. The program will capture **3 directions** (`x`, `y`, `z`) and save images under `data/saved_obj_img/<object_name>/`.  
3. Repeat step 2 for more objects.  
4. Click **Abstract** (right panel) to extract features, which will create `data/extract_saved_obj_feature/obj_features.pt`.  
5. Click **Recognize** to run detection/segmentation and show similarity scores per detected region. Results are saved to `data/recognized_results/`.

#### ğŸ’¡ Controls  
- **Enter**: confirm the typed object name and start capturing images  
- **Buttons**: `Abstract` (feature extraction), `Recognize` (run recognition), `Exit` (quit)  
- **ESC**: quit (when idle/Waiting)  

#### ğŸ’¡ Outputs (summary)  
- Training images: `data/saved_obj_img/`  
- Extracted features: `data/extract_saved_obj_feature/`  
- Sample features: `data/sample_features/`  
- Test snapshot: `data/test_img/`  
- Recognition results: `data/recognized_results/`  
