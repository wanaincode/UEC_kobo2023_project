# ç”»åƒèªè­˜æŠ€è¡“ã‚’æ´»ç”¨ã—ãŸå†·è”µåº«å†…é£Ÿæè‡ªå‹•åˆ¤åˆ¥ã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™º   
## 1. Intro

- ðŸ† æƒ…å ±å‡¦ç†å­¦ä¼š ç¬¬86å›žå¤§ä¼š å­¦ç”Ÿå¥¨åŠ±è³ž å—è³žä½œå“  
- Final project for **"æƒ…å ±å·¥å­¦å·¥æˆ¿ (Information Engineering Workshop)"**, The University of Electro-Communications (é›»æ°—é€šä¿¡å¤§å­¦)  


### 1.1 ðŸ“¹ Demo  
[![Watch the demo](https://img.youtube.com/vi/WM_rVHsI6sQ/maxresdefault.jpg)](https://www.youtube.com/watch?v=WM_rVHsI6sQ)  

---

### 1.2 ðŸ§‘â€ðŸ’¼ Project Info  
- **Theme:** ã€Œå·¨å¤§ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã“ãªã›ï¼å¤§è¦æ¨¡æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸç”»åƒèªè­˜ãƒ»ç”Ÿæˆã€  
- **Authors:**  
  - Mengchi Wang (çŽ‹ å­Ÿçª)  
  - Junwen Chen (é™³ ä¿Šæ–‡) â€“ Teaching Assistant  
  - Prof. Keiji Yanai (æŸ³äº• å•“å¸) â€“ Faculty Advisor  

---

### 1.3 ðŸ” Overview  
This project integrates multiple state-of-the-art vision and language models to detect, segment, and recognize food items in images, and optionally fetch their names and calorie information using the OpenAI API.

Models used:  
- **GroundingDINO** â€“ open-vocabulary object detection with text prompts  
- **Segment Anything (SAM)** â€“ precise image segmentation  
- **CLIP** â€“ feature embedding and similarity matching  
- **OpenAI GPT-4 Vision** (optional) â€“ retrieve food names & calories  

Ideal for:  
- Food logging  
- AR food labeling  
- Diet tracking systems  

---

## 2. Architecture  

     +----------------+          +---------------------+
     |   Input Image  |  --->    |  GroundingDINO      |
     +----------------+          +---------------------+
                                          |
                                          v
                            +---------------------------+
                            | Segment Anything (SAM)    |
                            +---------------------------+
                                          |
                                          v
                            +---------------------------+
                            | CLIP Feature Extraction   |
                            +---------------------------+
                                          |
                  +--------------------+  v
                  | Feature Matching   | ---> Object Label
                  +--------------------+
                                          |
                                          v
                        (Optional) Call OpenAI GPT-4 API
                        to retrieve object name & calorie



### 2.1 ðŸ” Key Features
- Text-based object detection (GroundingDINO)  
- Image segmentation (SAM)  
- Embedding & similarity search (CLIP)  
- One-shot recognition with registered features  
- Optional: API-based food description & calories  
- Export visual results with bounding boxes & labels  

---

## 3. Getting Started  

### 3.1 Installation  
```bash
git clone https://github.com/wanaincode/KOBO2023_project.git
cd KOBO2023_project
pip install -r requirements.txt
```
Download checkpoints for:  
- GroundingDINO  
- SAM (ViT-H)  
- CLIP (ViT-B/32)  

All scripts expect read/write under `data/` (created automatically on first run).


### 3.2 Usage  

Choose one of the following modes depending on your preference and use case:

| Script                     | Interaction Type       | Detection & Recognition                      | Output & Paths (under `data/`)                                  | Notes |
|---------------------------|------------------------|-----------------------------------------------|------------------------------------------------------------------|-------|
| `demo_ui.py` (recommended)| OpenCV GUI (webcam)    | Real-time detect â†’ segment â†’ embed â†’ match    | Saves to `recognized_results/`; uses `sample_features/` or `extract_saved_obj_feature/` | Mouse buttons: Abstract / Recognize; Enter to capture |
| `demo_no_ui.py`           | Terminal + webcam      | Batch/one-shot via key commands               | Same `data/` layout (`saved_obj_img/`, `extract_saved_obj_feature/`, etc.) | Lightweight, no GUI |
| `demo_basic.py`           | Minimal (webcam)       | Basic CLIP similarity example                 | Writes simple outputs under `data/`                              | For quick testing |


#### ðŸ“‚ Directory Structure

```
KOBO2023_project/
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ api_get_food_info.py
â”‚   â”œâ”€â”€ demo_ui.py
â”‚   â”œâ”€â”€ demo_no_ui.py
â”‚   â”œâ”€â”€ opencv_frame.py
â”‚   â””â”€â”€ sam_generate_obj_mask.py
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â”œâ”€â”€ SAM_demo.ipynb
â”‚   â”œâ”€â”€ clip_test.ipynb
â”‚   â””â”€â”€ ground_dino_sam_clip.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ saved_obj_img/         # Your captured object images (organized by object name)
â”‚   â”œâ”€â”€ extract_saved_obj_feature/  # Auto-generated masks/crops & obj_features.pt
â”‚   â”œâ”€â”€ sample_features/       # Bundled sample obj_features.pt (for quick try)
â”‚   â”œâ”€â”€ test_img/              # Auto-saved test.jpg when recognizing
â”‚   â””â”€â”€ recognized_results/    # Prediction results (annotated images)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> Note: If you are running scripts directly from the repo root (not using `src/`), adjust the command accordingly (e.g., `python demo_ui.py`). All I/O paths are under the `data/` directory.

### 3.3 Running Recognition with `demo_ui.py` (GUI Version)

This is the recommended way to run the pipeline interactively with visual feedback. The GUI uses your webcam.

#### ðŸ’¡ Quick Try  
You can try recognition immediately using the bundled sample features:  
- The app will prefer `data/sample_features/obj_features.pt` if present;  
- Otherwise, it will fall back to `data/extract_saved_obj_feature/obj_features.pt`.  

A snapshot from the webcam will be saved under `data/test_img/`, and annotated results will be written to `data/recognized_results/`.

#### ðŸ’¡ Register Your Own Objects  
1. Run the GUI:
   ```bash
   python src/demo_ui.py
   ```  
2. In the bottom input box, type an **object name** (e.g., `apple`) and press **Enter**. The program will capture **3 directions** (`x`, `y`, `z`) and save images under `data/saved_obj_img/<object_name>/`.  
3. Repeat step 2 for more objects.  
4. Click **Abstract** (right panel) to extract features, which will create `data/extract_saved_obj_feature/obj_features.pt`.  
5. Click **Recognize** to run detection/segmentation and show similarity scores per detected region. Results are saved to `data/recognized_results/`.

#### ðŸ’¡ Controls  
- **Enter**: confirm the typed object name and start capturing images  
- **Buttons**: `Abstract` (feature extraction), `Recognize` (run recognition), `Exit` (quit)  
- **ESC**: quit (when idle/Waiting)  

#### ðŸ’¡ Outputs (summary)  
- Training images: `data/saved_obj_img/`  
- Extracted features: `data/extract_saved_obj_feature/`  
- Sample features: `data/sample_features/`  
- Test snapshot: `data/test_img/`  
- Recognition results: `data/recognized_results/`  

---
