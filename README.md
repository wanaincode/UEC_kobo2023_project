ğŸ± AI Food Recognizer with GroundingDINO + SAM + CLIP

This project is the final work for the elective course â€œæƒ…å ±å·¥å­¦å·¥æˆ¿ (Information Engineering Workshop)â€ at The University of Electro-Communications (é›»æ°—é€šä¿¡å¤§å­¦), under the I-Class curriculum for Information Science students.

ğŸ§‘â€ğŸ’¼ Theme:
ã€Œå·¨å¤§ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã“ãªã›ï¼å¤§è¦æ¨¡æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸç”»åƒèªè­˜ãƒ»ç”Ÿæˆã€
(â€œMaster the Giant Models! Image Recognition and Generation with Large-Scale Deep Learning Modelsâ€)

ğŸ‘¨â€ğŸ’» Authors:
	â€¢	MengChi Wang (ç‹ å­Ÿçª)
	â€¢	Junwen Chen (é™³ ä¿Šæ–‡) â€“ Teaching Assistant
	â€¢	Prof. Keiji Yanai (æŸ³äº• å•“å¸) â€“ Faculty Advisor

â¸»

This project integrates multiple state-of-the-art vision and language models to detect, segment, and recognize food items in images, and optionally fetch their names and calorie information using the OpenAI API.

â¸»

ğŸ” Overview

This pipeline combines:
	â€¢	GroundingDINO â€“ for open-vocabulary object detection with text prompts
	â€¢	Segment Anything (SAM) â€“ for precise image segmentation
	â€¢	CLIP â€“ for embedding extracted image regions into a semantic space
	â€¢	OpenAI GPT-4 API â€“ to describe the recognized object and return its calorie (optional)

Ideal for building intelligent food loggers, AR food labeling, and diet tracking systems.

â¸»

ğŸ§  Architecture

     +----------------+          +---------------------+
     |   Input Image  |  --->    |  GroundingDINO      |
     +----------------+          +---------------------+
                                          |
                                          v
                            +---------------------------+
                            | Segment Anything (SAM)    |
                            +---------------------------+
                                          |
                                          v
                            +---------------------------+
                            | CLIP Feature Extraction   |
                            +---------------------------+
                                          |
                  +--------------------+  v
                  | Feature Matching   | ---> Object Label
                  +--------------------+
                                          |
                                          v
                        (Optional) Call OpenAI GPT-4 API
                        to retrieve object name & calorie



â¸»

ğŸ“Œ Features
	â€¢	ğŸ” Text-based object detection using GroundingDINO
	â€¢	ğŸ­ Image segmentation using SAM (Segment Anything)
	â€¢	ğŸ§  Feature embedding and similarity matching with CLIP
	â€¢	ğŸ“Š Optional: retrieve object description and calories via OpenAI GPT-4 Vision
	â€¢	ğŸ“€ Feature registration and one-shot object recognition
	â€¢	ğŸ–¼ Exportable visual results with bounding boxes and labels

â¸»

ğŸš€ Getting Started

1. Install Dependencies

Install Python dependencies and download model checkpoints for:
	â€¢	GroundingDINO
	â€¢	SAM (ViT-H)
	â€¢	CLIP (ViT-B/32)
	â€¢	OpenAI SDK (optional)

pip install -r requirements.txt



â¸»

2. Register Objects

Place your categorized object images in the saved_obj_img/ folder:

saved_obj_img/
â”œâ”€â”€ apple/
â”‚   â”œâ”€â”€ img1.jpg
â”‚   â””â”€â”€ img2.jpg
â”œâ”€â”€ banana/
    â””â”€â”€ img1.jpg

Then run:

python main.py

This will extract features and save them to extract_saved_obj_feature/.

â¸»

3. Run Recognition

Put test images in test_img/, and run the recognition pipeline:

recognize_pipeline(model, predictor, extractor, obj_features, './test_img/example.jpg', box_threshold=0.15, text_threshold=0.15)

The results will be saved under recognized_results/.

â¸»

ğŸ“‚ Directory Structure

.
â”œâ”€â”€ GroundingDINO/                # GroundingDINO model & configs
â”œâ”€â”€ segment-anything/             # SAM model
â”œâ”€â”€ saved_obj_img/                # Registered training images
â”œâ”€â”€ extract_saved_obj_feature/    # Feature vectors
â”œâ”€â”€ test_img/                     # Input test images
â”œâ”€â”€ recognized_results/           # Prediction result images
â”œâ”€â”€ main.py                       # Main logic script
â””â”€â”€ utils/                        # (Optional) helper functions



â¸»

ğŸ”‘ API Integration (Optional)

To fetch object name and calorie:

1. Set your OpenAI API key:

```python
client = OpenAI(api_key="your-api-key")
```

2.	The model sends a base64-encoded cropped image to GPT-4 Vision, requesting name and calorie in a defined format.

â¸»

ğŸ“¸ Example Results

Original Image	Recognized Output
	

Note: Add your own example images under examples/ folder.

â¸»

ğŸ“„ License

This project is released under the MIT License.

